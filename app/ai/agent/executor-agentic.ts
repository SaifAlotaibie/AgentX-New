/**
 * TRUE AGENTIC EXECUTOR
 * LLM makes ALL decisions about tool calling autonomously
 */

import { streamText, stepCountIs } from 'ai'
import { agentModel } from './groq-client'
import { AGENTIC_TOOLS } from './tools-agentic'
import AGENT_SYSTEM_PROMPT from './system_prompt'
import { saveConversation } from '@/lib/db/conversationService'
import { updateUserBehavior, logAgentAction } from '../tools/logger'
import { executeProactiveEngineForUser, getProactiveEventsForUser } from '../proactive'
import { getCachedProactiveData, setCachedProactiveData } from './proactive-cache'
import { db } from '@/lib/db/db'

interface AgentMessage {
  role: 'user' | 'assistant' | 'system'
  content: string
}

interface AgentResponse {
  response: any
  isStream: boolean
  tools_used?: string[]
}

/**
 * ðŸ”‘ Wrap tools to inject userId automatically
 * LLM doesn't need to provide user_id - we inject it from backend context
 */
function wrapToolsWithUserId(tools: any, userId: string) {
  const wrappedTools: any = {}

  for (const [toolName, toolDef] of Object.entries(tools)) {
    const originalExecute = (toolDef as any).execute

    wrappedTools[toolName] = {
      ...(toolDef as any),
      execute: async (params: any) => {
        // Auto-inject user_id if the tool needs it but LLM didn't provide it
        const finalParams = { ...params }

        // If this tool needs user_id and it's missing/undefined or dummy, inject it
        if (!finalParams.user_id || finalParams.user_id === 'undefined' || finalParams.user_id === '00000000-0000-0000-0000-000000000000') {
          finalParams.user_id = userId
        }

        console.log(`ðŸ”§ [AUTO-INJECT] ${toolName} with user_id:`, userId)
        return await originalExecute(finalParams)
      }
    }
  }

  return wrappedTools
}

/**
 * TRUE AGENTIC AGENT EXECUTOR
 * - LLM decides which tools to call
 * - LLM determines parameters
 * - LLM chains multiple tools autonomously
 */
export async function executeAgenticAgent(
  userMessage: string,
  userId: string,
  conversationHistory: AgentMessage[] = []
): Promise<AgentResponse> {
  try {
    console.log('ðŸ¤– [AGENTIC] Received message:', userMessage)
    console.log('ðŸ‘¤ [AGENTIC] User ID:', userId)

    // Step 1: PROACTIVE ENGINE - Get events and predictions (with caching!)
    let proactiveResult: any
    let pendingEvents: any[]

    const cached = getCachedProactiveData(userId)
    if (cached) {
      // Use cached data (saves 2-5 seconds!)
      pendingEvents = cached.events
      proactiveResult = cached.predictions
      console.log(`âš¡ [CACHE HIT] Using cached proactive data`)
    } else {
      // Run fresh proactive engine
      console.log('ðŸ”® [AGENTIC] Running Proactive Engine...')
      proactiveResult = await executeProactiveEngineForUser(userId)
      pendingEvents = await getProactiveEventsForUser(userId, 3)

      // Cache for 5 minutes
      setCachedProactiveData(userId, pendingEvents, proactiveResult)
    }

    console.log(`ðŸŽ¯ [AGENTIC] Proactive: ${pendingEvents.length} events, ${proactiveResult.predictions?.size || 0} predictions`)

    // Step 2: Get user profile for personalized greetings
    const { data: userProfile } = await db
      .from('user_profile')
      .select('full_name')
      .eq('user_id', userId)
      .single()

    // Step 3: Build conversation context with proactive information
    const messages = buildMessages(
      userMessage,
      userId,
      conversationHistory,
      pendingEvents,
      proactiveResult,
      userProfile?.full_name || null
    )

    console.log('ðŸ§  [AGENTIC] Calling Groq GPT-OSS-120B with tool support...')
    console.log('ðŸ”§ [AGENTIC] Available tools:', Object.keys(AGENTIC_TOOLS).length)

    // ðŸ”‘ Wrap tools to auto-inject user_id
    const toolsWithUserId = wrapToolsWithUserId(AGENTIC_TOOLS, userId)

    // Step 3: LET THE LLM DECIDE - Stream with tool support
    const result = streamText({
      model: agentModel, // openai/gpt-oss-120b
      messages: messages as any,
      tools: toolsWithUserId, // ðŸŽ¯ Tools with auto-injected user_id!
      temperature: 0.3, // Lower temp for more reliable tool calling
      stopWhen: stepCountIs(5), // ðŸ”‘ CRITICAL FIX: Allows model to continue after tools and generate text response
      // Default is stepCountIs(1) which stops immediately after tool execution WITHOUT text generation!

      // Callback when each step finishes
      onStepFinish: async (step) => {
        if (step.toolCalls && step.toolCalls.length > 0) {
          console.log('ðŸ”§ [AGENTIC] LLM called tools:',
            step.toolCalls.map(tc => tc.toolName).join(', '))
        }
      },

      // Callback when complete
      async onFinish({ text, toolCalls, toolResults }) {
        console.log('âœ… [AGENTIC] Stream finished')
        console.log('ðŸ“ [DEBUG] Response text:', text ? `"${text}"` : 'EMPTY')
        console.log('ðŸ“ [DEBUG] Text length:', text?.length || 0)

        // ðŸ”¥ DEBUG: If tools were called but no text was generated
        if (toolCalls && toolCalls.length > 0 && (!text || text.trim().length === 0)) {
          console.error('âš ï¸ [WARNING] Tools were called but NO text response was generated!')
          console.error('âš ï¸ This usually means stopWhen is set to stepCountIs(1) (default behavior)')
          console.error('âš ï¸ Tool results:', JSON.stringify(toolResults?.slice(0, 2), null, 2))
        }

        const toolsUsed = toolCalls?.map(tc => tc.toolName) || []
        console.log('ðŸ”§ [AGENTIC] Total tools used:', toolsUsed.length, toolsUsed)

        // ðŸ”¥ PERFORMANCE: Fire-and-forget DB writes (don't block response)
        const intent = detectIntentFromToolCalls(toolCalls)

        // Save conversation (non-blocking)
        saveConversation(userId, 'user', userMessage).catch(err =>
          console.error('Error saving user message:', err))
        saveConversation(userId, 'assistant', text).catch(err =>
          console.error('Error saving assistant message:', err))

        // Update user behavior (non-blocking)
        updateUserBehavior(userId, {
          last_message: userMessage,
          intent: intent,
          predicted_need: proactiveResult?.predictions?.get(userId)?.predicted_need || null
        }).catch(err => console.error('Error updating user behavior:', err))

        // Log agent action (non-blocking)
        logAgentAction(userId, 'agentic_chat', {
          message: userMessage,
          intent,
          tools_called: toolsUsed,
          proactive_events: pendingEvents.length
        }, {
          response: text,
          tool_count: toolCalls?.length || 0,
          tool_results: toolResults?.map(tr => ({ success: (tr as any)?.success || false }))
        }).catch(err => console.error('Error logging agent action:', err))

        console.log('ðŸ’¾ [AGENTIC] DB writes queued (non-blocking)')
      }
    })

    return {
      response: result,
      isStream: true,
      tools_used: []
    }

  } catch (error: any) {
    console.error('âŒ [AGENTIC] Error:', error)
    throw error
  }
}

/**
 * Build messages array with proactive context
 */
function buildMessages(
  userMessage: string,
  userId: string,
  history: AgentMessage[],
  pendingEvents: any[],
  proactiveResult: any,
  userName: string | null
): any[] {
  // Start with hybrid system prompt (balanced: comprehensive yet efficient)
  let systemPrompt = AGENT_SYSTEM_PROMPT

  // Add user name context if available
  if (userName) {
    systemPrompt += `\n\n**Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…**: Ø§Ù„Ø§Ø³Ù…: ${userName}\n`
  }

  // Note: user_id is automatically injected by the wrapper function
  // LLM doesn't need to worry about it!

  // Helper to get friendly Arabic labels for technical keys (used by both events and predictions)
  const getArabicLabel = (key: string): string => {
    const staticLabels: Record<string, string> = {
      // Events
      'contract_expiring_soon': 'ØªÙ†Ø¨ÙŠÙ‡: Ù‚Ø±Ø¨ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ø¹Ù‚Ø¯',
      'upcoming_appointment': 'ØªØ°ÙƒÙŠØ±: Ù…ÙˆØ¹Ø¯ Ù‚Ø§Ø¯Ù…',
      'ticket_follow_up_needed': 'Ù…ØªØ§Ø¨Ø¹Ø©: ØªØ°ÙƒØ±Ø© Ù…ÙØªÙˆØ­Ø©',
      'user_dissatisfaction_detected': 'ØªÙ†Ø¨ÙŠÙ‡: Ø§Ù†Ø®ÙØ§Ø¶ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø±Ø¶Ø§',
      'incomplete_resume_detected': 'ØªÙ†Ø¨ÙŠÙ‡: Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°Ø§ØªÙŠØ© ØºÙŠØ± Ù…ÙƒØªÙ…Ù„Ø©',

      // Predictions
      'urgent_support_needed': 'Ø¯Ø¹Ù… Ø¹Ø§Ø¬Ù„ Ù…Ø·Ù„ÙˆØ¨',
      'contract_renewal': 'ØªØ¬Ø¯ÙŠØ¯ Ø¹Ù‚Ø¯',
      'certificate_request': 'Ø·Ù„Ø¨ Ø´Ù‡Ø§Ø¯Ø©',
      'appointment_preparation': 'ØªØ¬Ù‡ÙŠØ² Ù„Ù…ÙˆØ¹Ø¯',
      'ticket_follow_up': 'Ù…ØªØ§Ø¨Ø¹Ø© ØªØ°Ø§ÙƒØ±',
      'general_inquiry': 'Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ø§Ù…'
    }

    if (staticLabels[key]) return staticLabels[key]

    // Dynamic Predictions
    if (key.startsWith('frequent_')) {
      if (key.includes('certificates')) return 'Ù…Ø³ØªØ®Ø¯Ù… Ù†Ø´Ø· Ù„Ù„Ø´Ù‡Ø§Ø¯Ø§Øª'
      if (key.includes('contracts')) return 'Ù…Ø³ØªØ®Ø¯Ù… Ù†Ø´Ø· Ù„Ù„Ø¹Ù‚ÙˆØ¯'
      if (key.includes('resumes')) return 'Ù…Ø³ØªØ®Ø¯Ù… Ù†Ø´Ø· Ù„Ù„Ø³ÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠØ©'
    }

    if (key.startsWith('interested_in_')) {
      if (key.includes('contracts')) return 'Ù…Ù‡ØªÙ… Ø¨Ø§Ù„Ø¹Ù‚ÙˆØ¯'
      if (key.includes('certificates')) return 'Ù…Ù‡ØªÙ… Ø¨Ø§Ù„Ø´Ù‡Ø§Ø¯Ø§Øª'
      if (key.includes('appointments')) return 'Ù…Ù‡ØªÙ… Ø¨Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯'
      if (key.includes('tickets')) return 'Ù…Ù‡ØªÙ… Ø¨Ø§Ù„ØªØ°Ø§ÙƒØ±'
      if (key.includes('resumes')) return 'Ù…Ù‡ØªÙ… Ø¨Ø§Ù„Ø³ÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠØ©'
      if (key.includes('courses')) return 'Ù…Ù‡ØªÙ… Ø¨Ø§Ù„Ø¯ÙˆØ±Ø§Øª'
      if (key.includes('feedback')) return 'Ù…Ù‡ØªÙ… Ø¨Ø§Ù„ØªÙ‚ÙŠÙŠÙ…'
    }

    return key // Fallback
  }

  // Add proactive context if available
  if (pendingEvents.length > 0) {
    systemPrompt += '\n\n## ðŸ”” Ø£Ø­Ø¯Ø§Ø« Ø§Ø³ØªØ¨Ø§Ù‚ÙŠØ© ØªØ­ØªØ§Ø¬ Ø§Ù†ØªØ¨Ø§Ù‡Ùƒ:\n'
    systemPrompt += '**Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ù…Ø¹Ù„Ù‚Ø©:**\n'

    // Deduplicate events by type (avoid showing "Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°Ø§ØªÙŠØ© ØºÙŠØ± Ù…ÙƒØªÙ…Ù„Ø©" 3 times)
    const uniqueEvents = new Map<string, any>()
    pendingEvents.forEach(event => {
      if (!uniqueEvents.has(event.event_type)) {
        uniqueEvents.set(event.event_type, event)
      }
    })

    let eventIndex = 1
    uniqueEvents.forEach(event => {
      const label = getArabicLabel(event.event_type)
      systemPrompt += `${eventIndex}. ${label}: ${event.suggested_action}\n`
    })
    systemPrompt += '\nâš ï¸ Ù„Ø§ ØªØªØ¬Ø§Ù‡Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« - Ø§Ø°ÙƒØ±Ù‡Ø§ Ø¯Ø§Ø¦Ù…Ø§Ù‹ ÙÙŠ Ø±Ø¯Ùƒ Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ¯ÙŠØ©.\n'
    systemPrompt += `
# ðŸŽ­ Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø­Ø¯ÙŠØ«
- ØªØ­Ø¯Ø« Ø¨Ù„Ù‡Ø¬Ø© Ø³Ø¹ÙˆØ¯ÙŠØ© Ø¨ÙŠØ¶Ø§Ø¡ (Ø±Ø³Ù…ÙŠØ© Ù„ÙƒÙ† ÙˆØ¯ÙˆØ¯Ø©).
- ÙƒÙ† Ù…Ø®ØªØµØ±Ø§Ù‹ Ø¬Ø¯Ø§Ù‹. Ù„Ø§ ØªÙƒØªØ¨ ÙÙ‚Ø±Ø§Øª Ø·ÙˆÙŠÙ„Ø©.
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø¨Ø§Ø¹ØªØ¯Ø§Ù„ (âœ…ØŒ ðŸ“„ØŒ ðŸ””).
- Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø§Ø¹Ø±Ø¶ "Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©" Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù….
- **Ø¹Ù†Ø¯ Ø§Ù„ØªØ±Ø­ÙŠØ¨**: Ø¥Ø°Ø§ Ø±Ø­Ø¨ Ø¨Ùƒ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù…Ø«Ù„ "Ù…Ø±Ø­Ø¨Ø§" Ø£Ùˆ "Ø§Ù‡Ù„Ø§")ØŒ Ø±Ø¯ Ø¨Ø§Ø³Ù…Ù‡ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹ (Ù…Ø«Ø§Ù„: "Ø£Ù‡Ù„Ø§Ù‹ Ø¹Ø²Ø§Ù…!"). Ù‡Ø°Ø§ ÙŠØ¹Ø·ÙŠ ØªØ¬Ø±Ø¨Ø© Ø´Ø®ØµÙŠØ© ÙˆØ¯Ø§ÙØ¦Ø©.
`
  }

  // Add prediction context if available
  if (proactiveResult?.predictions?.size > 0) {
    const prediction = proactiveResult.predictions.get(userId)
    if (prediction && prediction.confidence > 0.6) {
      systemPrompt += `\n\n## ðŸŽ¯ ØªÙˆÙ‚Ø¹ Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª (Ø«Ù‚Ø© ${(prediction.confidence * 100).toFixed(0)}%):\n`
      systemPrompt += `Ø§Ù„ØªÙˆÙ‚Ø¹: ${getArabicLabel(prediction.predicted_need)}\n`
      systemPrompt += `Ø§Ù„Ø³Ø¨Ø¨: ${prediction.reasoning}\n`
    }
  }

  const messages = [
    { role: 'system', content: systemPrompt }
  ]

  // Add conversation history (last 5 messages)
  for (const msg of history.slice(-5)) {
    const content = extractMessageContent(msg)
    if (content) {
      messages.push({
        role: msg.role === 'assistant' ? 'assistant' : 'user',
        content
      })
    }
  }

  // Add current user message
  messages.push({
    role: 'user',
    content: userMessage
  })

  return messages
}

/**
 * Detect intent from tool calls made by LLM
 */
function detectIntentFromToolCalls(toolCalls: any[]): string {
  if (!toolCalls || toolCalls.length === 0) return 'general_inquiry'

  const toolNames = toolCalls.map(tc => tc.toolName)

  // Resume intents
  if (toolNames.includes('updateResume')) return 'update_resume'
  if (toolNames.includes('addCourse')) return 'add_course'
  if (toolNames.includes('getResume')) return 'view_resume'

  // Certificate intents
  if (toolNames.includes('createCertificate')) return 'certificate_request'
  if (toolNames.includes('getCertificates')) return 'view_certificates'

  // Contract intents
  if (toolNames.includes('renewContract')) return 'renew_contract'
  if (toolNames.includes('updateContract')) return 'update_contract'
  if (toolNames.includes('getContracts')) return 'view_contracts'

  // Ticket intents
  if (toolNames.includes('createTicket')) return 'ticket_creation'
  if (toolNames.includes('checkTicketStatus')) return 'check_ticket'

  // Appointment intents
  if (toolNames.includes('scheduleAppointment')) return 'book_appointment'
  if (toolNames.includes('cancelAppointment')) return 'cancel_appointment'
  if (toolNames.includes('getAppointments')) return 'view_appointments'

  return 'general_inquiry'
}

/**
 * Extract text content from message (handles UI SDK format)
 */
function extractMessageContent(msg: any): string {
  if (typeof msg.content === 'string') {
    return msg.content
  } else if (msg.parts && Array.isArray(msg.parts)) {
    const textPart = msg.parts.find((p: any) => p.type === 'text')
    return textPart?.text || ''
  }
  return ''
}
