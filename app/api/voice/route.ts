import { NextRequest, NextResponse } from "next/server"
import OpenAI from "openai"

export const runtime = "nodejs"

export async function POST(request: NextRequest) {
  try {
    const contentType = request.headers.get('content-type')
    
    // Handle greeting request
    if (contentType?.includes('application/json')) {
      const body = await request.json()
      
      if (body.action === 'greeting') {
        return NextResponse.json({
          userText: '',
          aiText: 'Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒØŒ Ù…Ø¹Ùƒ Ø®Ø¯Ù…Ø© Ø¹Ù…Ù„Ø§Ø¡ ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¨Ø´Ø±ÙŠØ© ÙˆØ§Ù„ØªÙ†Ù…ÙŠØ© Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©. ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ',
          audio: null,
        })
      }
    }

    // Process voice input
    const formData = await request.formData()
    const audioFile = formData.get('audio') as Blob
    const user_id = formData.get('user_id') as string
    
    if (!audioFile) {
      return NextResponse.json(
        { error: 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„ØµÙˆØª' },
        { status: 400 }
      )
    }

    const arrayBuffer = await audioFile.arrayBuffer()
    const audioBuffer = Buffer.from(arrayBuffer)

    if (audioBuffer.byteLength === 0) {
      return NextResponse.json({
        userText: '',
        aiText: 'Ø§Ù„ØµÙˆØª Ø§Ù„Ù…ÙØ±Ø³ÙÙ„ ÙØ§Ø±Øº. ØªØ£ÙƒØ¯ Ù…Ù† ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†.',
        audio: null,
      })
    }

    // Speech-to-Text using OpenAI Whisper
    const client = new OpenAI({
      baseURL: "https://api.openai.com/v1",
      apiKey: process.env.OPENAI_API_KEY,
    })

    const transcription = await client.audio.transcriptions.create({
      file: new File([audioBuffer], "recording.webm", { type: "audio/webm" }),
      model: "whisper-1",
      language: "ar",
    })

    const userText = transcription.text?.trim() || ""
    
    console.log('ğŸ¤', userText || 'EMPTY')

    if (!userText) {
      return NextResponse.json({
        userText: '',
        aiText: 'Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø³Ù…Ø§Ø¹Ùƒ Ø¨ÙˆØ¶ÙˆØ­.',
        audio: null,
      })
    }

    // Send to SILMA assistant
    const baseUrl = process.env.NEXT_PUBLIC_APP_URL || 'http://localhost:3000'
    
    const chatResponse = await fetch(`${baseUrl}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        message: userText,
        user_id: user_id,
        isVoiceRequest: true,
      }),
    })

    if (!chatResponse.ok) {
      throw new Error('Chat API failed')
    }

    const chatData = await chatResponse.json()
    const aiText = chatData.response || 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø±Ø¯.'

    console.log('ğŸ¤–', aiText.substring(0, 40) + '...')

    return NextResponse.json({
      userText,
      aiText,
      audio: null,
    })

  } catch (error: any) {
    console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”')
    console.error('âŒ Voice API error:', error.message)
    console.error('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”')
    
    const errorMessage = 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.'
    
    return NextResponse.json({
      userText: '',
      aiText: errorMessage,
      audio: null,
    }, { status: 200 })
  }
}
